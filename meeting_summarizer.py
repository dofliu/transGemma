# -*- coding: utf-8 -*-
"""
æœƒè­°æ‘˜è¦æ¨¡çµ„ (Meeting Summarizer Module)

åŠŸèƒ½ï¼š
1. å¾å½±ç‰‡æŠ½å–éŸ³è¨Š (ffmpeg)
2. èªéŸ³è½‰æ–‡å­— (faster-whisper)
3. AI æœƒè­°æ‘˜è¦ç”Ÿæˆ (Ollama / Gemini API)
"""

import os
import tempfile
import subprocess
from typing import Optional, Generator
from dataclasses import dataclass

# ========== è³‡æ–™é¡åˆ¥ ==========

@dataclass
class TranscriptSegment:
    """é€å­—ç¨¿ç‰‡æ®µ"""
    start: float
    end: float
    text: str


@dataclass
class MeetingSummaryResult:
    """æœƒè­°æ‘˜è¦çµæœ"""
    transcript: str                    # å®Œæ•´é€å­—ç¨¿
    transcript_with_time: str          # å¸¶æ™‚é–“æˆ³çš„é€å­—ç¨¿
    summary: dict                      # æ‘˜è¦å…§å®¹ {"key_points": ..., "action_items": ..., ...}
    duration: float                    # éŸ³è¨Šæ™‚é•·ï¼ˆç§’ï¼‰
    language: str                      # åµæ¸¬åˆ°çš„èªè¨€


# ========== æ‘˜è¦é¡å‹å®šç¾© ==========

SUMMARY_TYPES = {
    "key_points": {
        "name": "ğŸ“ æœƒè­°é‡é»",
        "prompt": "è«‹æ•´ç†é€™ä»½æœƒè­°é€å­—ç¨¿çš„é‡é»æ‘˜è¦ï¼Œç”¨æ¢åˆ—å¼å‘ˆç¾ä¸»è¦è¨è«–å…§å®¹ã€‚"
    },
    "action_items": {
        "name": "âœ… å¾…è¾¦äº‹é …",
        "prompt": "è«‹å¾æœƒè­°é€å­—ç¨¿ä¸­æå–æ‰€æœ‰å¾…è¾¦äº‹é …ï¼ˆAction Itemsï¼‰ï¼Œåˆ—å‡ºè² è²¬äººï¼ˆå¦‚æœ‰æåŠï¼‰å’Œæˆªæ­¢æ—¥æœŸï¼ˆå¦‚æœ‰æåŠï¼‰ã€‚"
    },
    "decisions": {
        "name": "ğŸ“‹ æ±ºè­°äº‹é …",
        "prompt": "è«‹å¾æœƒè­°é€å­—ç¨¿ä¸­æå–æ‰€æœ‰é”æˆçš„æ±ºè­°å’Œå…±è­˜ã€‚"
    },
    "full_summary": {
        "name": "ğŸ“„ å®Œæ•´æ‘˜è¦",
        "prompt": "è«‹ç‚ºé€™ä»½æœƒè­°é€å­—ç¨¿æ’°å¯«ä¸€ä»½å®Œæ•´çš„æœƒè­°æ‘˜è¦ï¼ŒåŒ…å«ï¼šæœƒè­°ä¸»é¡Œã€åƒèˆ‡è¨è«–çš„é‡é»ã€ä¸»è¦æ±ºè­°ã€ä»¥åŠå¾ŒçºŒè¡Œå‹•äº‹é …ã€‚"
    }
}


# ========== æœƒè­°æ‘˜è¦æœå‹™ ==========

class MeetingSummarizer:
    """æœƒè­°æ‘˜è¦æœå‹™"""
    
    def __init__(self, 
                 ai_backend: str = "ollama",
                 ollama_model: str = "qwen3:4b",
                 gemini_api_key: str = ""):
        """
        åˆå§‹åŒ–æœƒè­°æ‘˜è¦æœå‹™
        
        Args:
            ai_backend: "ollama" æˆ– "gemini"
            ollama_model: Ollama æ¨¡å‹åç¨±
            gemini_api_key: Gemini API Keyï¼ˆåƒ… gemini å¾Œç«¯éœ€è¦ï¼‰
        """
        self.ai_backend = ai_backend
        self.ollama_model = ollama_model
        self.gemini_api_key = gemini_api_key
        self._whisper_model = None
    
    def _get_whisper_model(self):
        """å»¶é²è¼‰å…¥ Whisper æ¨¡å‹"""
        if self._whisper_model is None:
            from faster_whisper import WhisperModel
            # ä½¿ç”¨ base æ¨¡å‹ï¼Œå¹³è¡¡é€Ÿåº¦èˆ‡æº–ç¢ºåº¦
            self._whisper_model = WhisperModel("base", device="cpu", compute_type="int8")
        return self._whisper_model
    
    # ========== éŸ³è¨ŠæŠ½å– ==========
    
    def extract_audio(self, video_path: str, output_dir: str = None) -> str:
        """
        å¾å½±ç‰‡æŠ½å–éŸ³è¨Š
        
        Args:
            video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘
            output_dir: è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ä½¿ç”¨æš«å­˜ç›®éŒ„ï¼‰
            
        Returns:
            éŸ³è¨Šæª”æ¡ˆè·¯å¾‘ (WAV æ ¼å¼)
        """
        if output_dir is None:
            output_dir = tempfile.mkdtemp(prefix="meeting_audio_")
        
        # ç”¢ç”Ÿè¼¸å‡ºè·¯å¾‘
        base_name = os.path.splitext(os.path.basename(video_path))[0]
        audio_path = os.path.join(output_dir, f"{base_name}_audio.wav")
        
        # ä½¿ç”¨ ffmpeg æŠ½å–éŸ³è¨Šä¸¦è½‰æ›ç‚º 16kHz mono WAVï¼ˆWhisper æœ€ä½³æ ¼å¼ï¼‰
        cmd = [
            "ffmpeg", "-y",
            "-i", video_path,
            "-vn",                    # ä¸è¦å½±ç‰‡
            "-acodec", "pcm_s16le",   # 16-bit PCM
            "-ar", "16000",           # 16kHz
            "-ac", "1",               # mono
            audio_path
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            return audio_path
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"éŸ³è¨ŠæŠ½å–å¤±æ•—: {e.stderr.decode()}")
    
    # ========== èªéŸ³è¾¨è­˜ ==========
    
    def transcribe(self, audio_path: str, language: str = "auto",
                   progress_callback=None) -> tuple[list[TranscriptSegment], str]:
        """
        èªéŸ³è½‰æ–‡å­—
        
        Args:
            audio_path: éŸ³è¨Šæª”æ¡ˆè·¯å¾‘
            language: èªè¨€ä»£ç¢¼ï¼ˆ"auto" è‡ªå‹•åµæ¸¬ï¼‰
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
            
        Returns:
            (segments, detected_language)
        """
        model = self._get_whisper_model()
        
        # èªè¨€å°æ‡‰
        whisper_lang_map = {
            "zh_TW": "zh", "zh_CN": "zh",
            "en_US": "en", "ja_JP": "ja", "ko_KR": "ko",
            "de_DE": "de", "fr_FR": "fr", "es_ES": "es",
            "it_IT": "it", "ru_RU": "ru", "pt_BR": "pt",
            "vi_VN": "vi", "th_TH": "th", "ar_SA": "ar",
        }
        
        lang_code = None if language == "auto" else whisper_lang_map.get(language, None)
        
        if progress_callback:
            progress_callback("ğŸ™ï¸ æ­£åœ¨é€²è¡ŒèªéŸ³è¾¨è­˜...")
        
        # åŸ·è¡Œè¾¨è­˜
        segments_iter, info = model.transcribe(
            audio_path, 
            language=lang_code,
            word_timestamps=False
        )
        
        # æ”¶é›†ç‰‡æ®µ
        segments = []
        for seg in segments_iter:
            segments.append(TranscriptSegment(
                start=seg.start,
                end=seg.end,
                text=seg.text.strip()
            ))
        
        return segments, info.language
    
    def format_transcript(self, segments: list[TranscriptSegment], 
                          with_timestamps: bool = True) -> str:
        """
        æ ¼å¼åŒ–é€å­—ç¨¿
        
        Args:
            segments: é€å­—ç¨¿ç‰‡æ®µåˆ—è¡¨
            with_timestamps: æ˜¯å¦åŒ…å«æ™‚é–“æˆ³
        """
        lines = []
        for seg in segments:
            if with_timestamps:
                start_time = self._format_time(seg.start)
                lines.append(f"[{start_time}] {seg.text}")
            else:
                lines.append(seg.text)
        
        return "\n".join(lines)
    
    def _format_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ™‚é–“ç‚º HH:MM:SS"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        
        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes:02d}:{secs:02d}"
    
    # ========== AI æ‘˜è¦ç”Ÿæˆ ==========
    
    def generate_summary(self, transcript: str, summary_types: list[str],
                         progress_callback=None) -> dict:
        """
        ç”Ÿæˆæœƒè­°æ‘˜è¦
        
        Args:
            transcript: å®Œæ•´é€å­—ç¨¿æ–‡å­—
            summary_types: æ‘˜è¦é¡å‹åˆ—è¡¨ ["key_points", "action_items", ...]
            progress_callback: é€²åº¦å›èª¿
            
        Returns:
            {"key_points": "...", "action_items": "...", ...}
        """
        if self.ai_backend == "gemini":
            return self._generate_summary_gemini(transcript, summary_types, progress_callback)
        else:
            return self._generate_summary_ollama(transcript, summary_types, progress_callback)
    
    def _generate_summary_ollama(self, transcript: str, summary_types: list[str],
                                  progress_callback=None) -> dict:
        """ä½¿ç”¨ Ollama ç”Ÿæˆæ‘˜è¦"""
        import ollama
        
        results = {}
        
        for i, summary_type in enumerate(summary_types):
            if summary_type not in SUMMARY_TYPES:
                continue
            
            type_info = SUMMARY_TYPES[summary_type]
            
            if progress_callback:
                progress_callback(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆ {type_info['name']} ({i+1}/{len(summary_types)})...")
            
            # æ§‹å»º prompt
            prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æœƒè­°è¨˜éŒ„æ•´ç†åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä»½æœƒè­°çš„é€å­—ç¨¿ï¼š

---
{transcript}
---

{type_info['prompt']}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæ ¼å¼æ¸…æ™°ã€æ¢ç†åˆ†æ˜ã€‚"""

            try:
                response = ollama.chat(
                    model=self.ollama_model,
                    messages=[{"role": "user", "content": prompt}],
                    options={"num_predict": 2048}
                )
                results[summary_type] = response['message']['content']
            except Exception as e:
                results[summary_type] = f"âŒ ç”Ÿæˆå¤±æ•—: {str(e)}"
        
        return results
    
    def _generate_summary_gemini(self, transcript: str, summary_types: list[str],
                                  progress_callback=None) -> dict:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆæ‘˜è¦"""
        try:
            import google.generativeai as genai
        except ImportError:
            return {st: "âŒ è«‹å®‰è£ google-generativeai: pip install google-generativeai" 
                    for st in summary_types}
        
        if not self.gemini_api_key:
            return {st: "âŒ è«‹æä¾› Gemini API Key" for st in summary_types}
        
        # é…ç½® Gemini
        genai.configure(api_key=self.gemini_api_key)
        model = genai.GenerativeModel("gemini-2.0-flash")
        
        results = {}
        
        for i, summary_type in enumerate(summary_types):
            if summary_type not in SUMMARY_TYPES:
                continue
            
            type_info = SUMMARY_TYPES[summary_type]
            
            if progress_callback:
                progress_callback(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆ {type_info['name']} ({i+1}/{len(summary_types)})...")
            
            # æ§‹å»º prompt
            prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æœƒè­°è¨˜éŒ„æ•´ç†åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä»½æœƒè­°çš„é€å­—ç¨¿ï¼š

---
{transcript}
---

{type_info['prompt']}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæ ¼å¼æ¸…æ™°ã€æ¢ç†åˆ†æ˜ã€‚"""

            try:
                response = model.generate_content(prompt)
                results[summary_type] = response.text
            except Exception as e:
                results[summary_type] = f"âŒ ç”Ÿæˆå¤±æ•—: {str(e)}"
        
        return results
    
    # ========== å®Œæ•´è™•ç†æµç¨‹ ==========
    
    def process_video(self, video_path: str, 
                      language: str = "auto",
                      summary_types: list[str] = None,
                      progress_callback=None) -> MeetingSummaryResult:
        """
        å®Œæ•´è™•ç†æµç¨‹ï¼šå½±ç‰‡ â†’ éŸ³è¨Š â†’ é€å­—ç¨¿ â†’ æ‘˜è¦
        
        Args:
            video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘
            language: èªè¨€ä»£ç¢¼ï¼ˆ"auto" è‡ªå‹•åµæ¸¬ï¼‰
            summary_types: æ‘˜è¦é¡å‹åˆ—è¡¨ï¼ˆé è¨­ç‚º ["full_summary"]ï¼‰
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
            
        Returns:
            MeetingSummaryResult
        """
        if summary_types is None:
            summary_types = ["full_summary"]
        
        # 1. æŠ½å–éŸ³è¨Š
        if progress_callback:
            progress_callback("ğŸ¬ æ­£åœ¨å¾å½±ç‰‡æŠ½å–éŸ³è¨Š...")
        
        audio_path = self.extract_audio(video_path)
        
        # 2. èªéŸ³è¾¨è­˜
        if progress_callback:
            progress_callback("ğŸ™ï¸ æ­£åœ¨é€²è¡ŒèªéŸ³è¾¨è­˜...")
        
        segments, detected_lang = self.transcribe(audio_path, language, progress_callback)
        
        # æ ¼å¼åŒ–é€å­—ç¨¿
        transcript = self.format_transcript(segments, with_timestamps=False)
        transcript_with_time = self.format_transcript(segments, with_timestamps=True)
        
        # è¨ˆç®—æ™‚é•·
        duration = segments[-1].end if segments else 0.0
        
        # 3. ç”Ÿæˆæ‘˜è¦
        if progress_callback:
            progress_callback("ğŸ¤– æ­£åœ¨ç”Ÿæˆæœƒè­°æ‘˜è¦...")
        
        summary = self.generate_summary(transcript, summary_types, progress_callback)
        
        # 4. æ¸…ç†æš«å­˜æª”æ¡ˆ
        try:
            os.remove(audio_path)
            os.rmdir(os.path.dirname(audio_path))
        except:
            pass
        
        return MeetingSummaryResult(
            transcript=transcript,
            transcript_with_time=transcript_with_time,
            summary=summary,
            duration=duration,
            language=detected_lang
        )
    
    def process_video_stream(self, video_path: str,
                             language: str = "auto",
                             summary_types: list[str] = None,
                             progress_callback=None) -> Generator[dict, None, None]:
        """
        ä¸²æµè™•ç†æµç¨‹ï¼Œé€æ­¥å›å‚³çµæœ
        
        Yields:
            {"stage": "...", "progress": 0.0-1.0, "data": ...}
        """
        if summary_types is None:
            summary_types = ["full_summary"]
        
        # Stage 1: æŠ½å–éŸ³è¨Š
        yield {"stage": "extract_audio", "progress": 0.1, "message": "ğŸ¬ æ­£åœ¨å¾å½±ç‰‡æŠ½å–éŸ³è¨Š..."}
        audio_path = self.extract_audio(video_path)
        yield {"stage": "extract_audio", "progress": 0.2, "message": "âœ… éŸ³è¨ŠæŠ½å–å®Œæˆ"}
        
        # Stage 2: èªéŸ³è¾¨è­˜
        yield {"stage": "transcribe", "progress": 0.3, "message": "ğŸ™ï¸ æ­£åœ¨é€²è¡ŒèªéŸ³è¾¨è­˜..."}
        segments, detected_lang = self.transcribe(audio_path, language)
        
        transcript = self.format_transcript(segments, with_timestamps=False)
        transcript_with_time = self.format_transcript(segments, with_timestamps=True)
        duration = segments[-1].end if segments else 0.0
        
        yield {
            "stage": "transcribe", 
            "progress": 0.5, 
            "message": f"âœ… èªéŸ³è¾¨è­˜å®Œæˆï¼ˆ{self._format_time(duration)}ï¼‰",
            "transcript": transcript,
            "transcript_with_time": transcript_with_time,
            "language": detected_lang,
            "duration": duration
        }
        
        # Stage 3: ç”Ÿæˆæ‘˜è¦
        yield {"stage": "summarize", "progress": 0.6, "message": "ğŸ¤– æ­£åœ¨ç”Ÿæˆæœƒè­°æ‘˜è¦..."}
        
        summary = {}
        for i, summary_type in enumerate(summary_types):
            progress = 0.6 + (0.35 * (i + 1) / len(summary_types))
            type_name = SUMMARY_TYPES.get(summary_type, {}).get("name", summary_type)
            yield {"stage": "summarize", "progress": progress, "message": f"ğŸ¤– æ­£åœ¨ç”Ÿæˆ {type_name}..."}
            
            partial_summary = self.generate_summary(transcript, [summary_type])
            summary.update(partial_summary)
            
            yield {
                "stage": "summarize",
                "progress": progress,
                "message": f"âœ… {type_name} å®Œæˆ",
                "partial_summary": {summary_type: partial_summary.get(summary_type, "")}
            }
        
        # æ¸…ç†
        try:
            os.remove(audio_path)
            os.rmdir(os.path.dirname(audio_path))
        except:
            pass
        
        yield {
            "stage": "done",
            "progress": 1.0,
            "message": "âœ… è™•ç†å®Œæˆï¼",
            "summary": summary
        }


# ========== å–®ä¾‹å¯¦ä¾‹ï¼ˆæ–¹ä¾¿ç›´æ¥ä½¿ç”¨ï¼‰==========

meeting_summarizer = MeetingSummarizer()
